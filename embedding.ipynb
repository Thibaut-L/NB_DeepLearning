{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Se familiariser avec les embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras met √† disposition plusieurs datasets, on va utiliser l'IMDB dataset üé¨:\n",
    "- Chaque document est une ***review d'un film***. \n",
    "- Chaque review est li√©e √† un score donn√©e par le spectateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/thibautlacroix/Documents/ENSC/3A/Parcours IA/NB_DL/embedding.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thibautlacroix/Documents/ENSC/3A/Parcours%20IA/NB_DL/embedding.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m###########################################\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thibautlacroix/Documents/ENSC/3A/Parcours%20IA/NB_DL/embedding.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m### Charger les donn√©es ###\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thibautlacroix/Documents/ENSC/3A/Parcours%20IA/NB_DL/embedding.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m###########################################\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thibautlacroix/Documents/ENSC/3A/Parcours%20IA/NB_DL/embedding.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thibautlacroix/Documents/ENSC/3A/Parcours%20IA/NB_DL/embedding.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m text_to_word_sequence\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thibautlacroix/Documents/ENSC/3A/Parcours%20IA/NB_DL/embedding.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(percentage_of_sentences\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Charger les donn√©es ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚ùì Regarder la forme des donn√©es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELS**: classification binaire:\n",
    "- label 0Ô∏è‚É£ correspond √† une review <font color=red>negative</font> \n",
    "- label 1Ô∏è‚É£ correspond √† une review <font color=green>positive</font> \n",
    "\n",
    "**INPUTS**: \n",
    "- üßπ Les donn√©es ont d√©j√† suivi un preprocessing\n",
    "\n",
    "‚ùì Tokeniser le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Imprimer quelques sequences pour observer le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "sentence_number = 100\n",
    "\n",
    "input_raw = X_train[sentence_number]\n",
    "input_token = X_train_token[sentence_number]\n",
    "\n",
    "for i in range(40):\n",
    "    print(f'Word : {input_raw[i]} -> Token {input_token[i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire qui fait le mapping de chaque token est accesible dans `tokenizer.word_index`\n",
    "    \n",
    "‚ùì Cr√©er une variable `vocab_size` qui contient le nombre de mots differentes dans le corpus d'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les sequences dans `X_train_token` et `X_test_token` ont des longueurs differentes.\n",
    "\n",
    "<img src=\"padding.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "Le padding est necessaire.\n",
    "\n",
    "‚ùì Utiliser l'utilit√© de tensor flow pour faire du padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Ecrire un mod√®le avec:\n",
    "- une couche d'embedding, les parametres: `input_dim` est la taille du vocabulaire (= `vocab_size`), et `output_dim` la dimension de l'embedding\n",
    "- une couche RNN (SimpleRNN, LSTM, GRU)\n",
    "- une couche dense\n",
    "- une couche de sortie\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Piste</summary>\n",
    "\n",
    "`input_dim` doit √™tre √©gal √† la taille du vocabulaire + 1 \n",
    "\n",
    "</details>\n",
    "\n",
    "Compiler le mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Regarder le nombre des param√®tres dans le mod√®le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Entra√Æner le mod√®le avec un early stopping avec patience = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Pour essayer d'optimiser le temps d'entra√Ænement, regarder la distribution de la taille de chaque document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_hist(X):\n",
    "    len_ = [len(_) for _ in X]\n",
    "    plt.hist(len_)\n",
    "    plt.title('Histogram of the number of sentences that have a given number of words')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90 - 95% des documents ont moins de 300 mots.\n",
    "\n",
    "Comme on n'a pas sp√©cifi√© `maxlen` dans le padding le tensor a la dimension egal a la taille du document le plus long. (+1000 mots)\n",
    "\n",
    "<img src=\"tensor_size.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "‚ùì Refaire le padding avec un `maxlen` de 200 et entra√Æner √† nouveau le mod√®le. Comparer la vitesse d'entra√Ænement et la performance du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
