{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è Utiliser [Gensim - Word2Vec](https://radimrehurek.com/gensim/auto_examples/index.html) ‚â• 4.0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras met √† disposition plusieurs datasets, on va utiliser l'IMDB dataset üé¨:\n",
    "- Chaque document est une ***review d'un film***. \n",
    "- Chaque review est li√©e √† un score donn√©e par le spectateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Charger les donn√©es ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"layers_embedding.png\" width=\"400px\" />\n",
    "\n",
    "L'entra√Ænement de l'embedding ajoute de la complexit√© et augmente le temps total d'entra√Ænement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "On va tester l'embedding avec Word2Vec.\n",
    "\n",
    "L'inconvenient est que l'embedding n'est pas entr√Æn√© specifiquement pour notre t√¢che,  MAIS, il y a des avantages:\n",
    "- rapidit√© d'embedding\n",
    "- l'embedding est quand m√™me representatif\n",
    "- la convergence du RNN sera plus rapide et facile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding  avec Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Embedding](word_embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Faire de l'embedding sur le dataset d'entra√Ænement avec word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Regarder la representation de quelques mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Quelle est la dimension de l'embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßê Comment juger la qualit√© de l'embedding?\n",
    "\n",
    "üí° Regarder si des mots proches semantiquement sont proches dans l'espace vectoriel\n",
    "\n",
    "üëâ [**`Word2Vec.wv.most_similar`**](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) est une methode qui retourne les mots les plus proches dans l'espace vectoriel √† l'input donn√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Tester la methode `most_similar` en quelques mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√©thode [**`similar_by_vector`**](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similar_by_vector) existe aussi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Op√©rations vectorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W2V(good) - W2V(bad)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì R√©aliser cette op√©ration et afficher le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "wv['good'] - wv['bad']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginons que:\n",
    "\n",
    "$$W2V(good) - W2V(bad) = W2V(nice) - W2V(stupid)$$\n",
    "\n",
    "Ce qui est equivalent √†:\n",
    "\n",
    "$$W2V(good) - W2V(bad) + W2V(stupid) = W2V(nice)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì R√©aliser cette operation et sauvegarder le r√©sultat dans une variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Utiliser `similar_by_vector` pour regarder les mots les plus proches dans l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Essayer \n",
    "\n",
    "$$W2V(Boy) - W2V(Girl) = W2V(Man) - W2V(Woman)$$\n",
    "\n",
    "ou \n",
    "\n",
    "$$W2V(Queen) - W2V(King) = W2V(actress) - W2V(actor)$$\n",
    "\n",
    "‚ùó Si les r√©sultats sont pas parfaites faut pas oublier la taille du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì `vector_size` correspond √† la dimension de l'embedding. Entra√Æner un nouveau mod√®le `word2vec_2` avec `X_train` mais avec `vector_size` diff√©rent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Utiliser **`Word2Vec.wv.key_to_index`** pour afficher la taille du vocabulaire appris. Compare avec les mots en `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a une diff√©rence importante entre le nombre de mots dans les phrases d'entra√Ænement et dans le vocabulaire de Word2Vec, m√™me si ce dernier a √©t√© entra√Æn√© sur l'ensemble des phrases d'entra√Ænement. La raison vient du deuxi√®me hyperparam√®tre important de Word2Vec : `min_count`. \n",
    "\n",
    "`min_count` est un entier qui vous indique combien d'occurrences un mot donn√© doit avoir pour √™tre appris dans l'espace d'int√©gration. Par exemple, disons que le mot \"movie\" appara√Æt 1000 fois dans le corpus et \"simba\" seulement 2 fois. Si `min_count=3`, le mot \"simba\" sera ignor√© pendant l'apprentissage.\n",
    "\n",
    "L'objectif est d'apprendre une repr√©sentation des mots qui sont suffisamment pr√©sents dans le corpus pour avoir une repr√©sentation int√©gr√©e robuste.\n",
    "\n",
    "Traduit avec www.DeepL.com/Translator (version gratuite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Apprenez un nouveau mod√®le `word2vec_3` avec un `min_count` sup√©rieur √† 5 (qui est la valeur par d√©faut) et un `word2vec_4` avec un `min_count` inf√©rieur √† 5, puis comparez la taille du vocabulaire pour tous les diff√©rents `word2vec` que vous avez appris (vous pouvez choisir n'importe quelle `vector_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappelons que Word2Vec dispose d'un r√©seau neuronal interne qui est optimis√© sur la base de certaines pr√©dictions. Ces pr√©dictions correspondent en fait √† la pr√©diction d'un mot en fonction des mots environnants. Les mots environnants se trouvent dans une \"fen√™tre\" qui correspond au nombre de mots pris en compte. Vous pouvez entra√Æner Word2Vec avec diff√©rentes tailles de \"fen√™tres\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Entra√Æner un nouveau mod√®le `word2vec_5` avec une `fen√™tre` diff√©rente de la pr√©c√©dente (la valeur par d√©faut est 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les arguments que vous avez vu (`vector_size`, `min_count` et `window`) sont g√©n√©ralement ceux avec lesquels vous devriez commencer √† jouer pour obtenir de meilleures performances pour votre mod√®le.\n",
    "\n",
    "Mais vous pouvez aussi regarder d'autres arguments dans la [**üìö Documentation - gensim.models.word2vec.Text8Corpus**](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir nos ensembles d'entra√Ænement et de test en ensembles de donn√©es pr√™ts pour le RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappelez-vous que `Word2Vec` est la premi√®re √©tape du processus global d'introduction d'une telle repr√©sentation dans un RNN, comme illustr√© ici :\n",
    "\n",
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "\n",
    "\n",
    "Passons maintenant √† l'√©tape 2 en convertissant les donn√©es d'entra√Ænement et de test en leur repr√©sentation vectorielle afin qu'elles soient pr√™tes √† √™tre introduites dans les RNN.\n",
    "\n",
    "‚ùì Maintenant, √©crire une fonction qui, √©tant donn√© une phrase, renvoie une matrice qui correspond √† l'embedding de la phrase compl√®te, ce qui signifie que vous devez int√©grer chaque mot l'un apr√®s l'autre et concat√©ner le r√©sultat pour obtenir une matrice 2D (assurez-vous que votre r√©sultat est un tableau NumPy).\n",
    "\n",
    "‚ùó **Remarque** ‚ùó Vous remarquerez probablement que certains mots que vous essayez de convertir provoquent des erreurs car ils n'appartiennent pas au dictionnaire :\n",
    "\n",
    "- Pour le test ceci est normal, quelques mots ne seront pas presentes dans les donn√©es d'entra√Ænement et donc l'embedding est inconnu.\n",
    "- Pour le train le `min_count` fait que pas tous les mots aient une representation vectorielle.\n",
    "\n",
    "Dans tous les cas, il suffit de sauter les mots manquants ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = ['this', 'movie', 'is', 'the', 'worst', 'action', 'movie', 'ever']\n",
    "example_missing_words = ['this', 'movie', 'is', 'laaaaaaaaaame']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "### Checks\n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (8, 100))\n",
    "\n",
    "embedded_sentence_missing_words = embed_sentence(word2vec, example_missing_words)\n",
    "assert(type(embedded_sentence_missing_words) == np.ndarray)\n",
    "assert(embedded_sentence_missing_words.shape == (3, 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì √âcrire une fonction qui, √©tant donn√© une liste de phrases (chaque phrase √©tant une liste de mots/cha√Ænes), renvoie une liste de phrases int√©gr√©es (chaque phrase est une matrice). Appliquez cette fonction aux phrases de formation et de test\n",
    "\n",
    "üí° _Piste_ : Utiliser la fonction pr√©c√©dente `embed_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "X_train = embedding(word2vec, X_train)\n",
    "X_test = embedding(word2vec, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Afin d'avoir des donn√©es pr√™tes √† l'emploi, n'oubliez pas le padding de vos s√©quences afin d'avoir des tenseurs qui peuvent √™tre divis√©s en lots (de `batch_size`) pendant l'optimisation. Stockez les valeurs padd√©es dans `X_train_pad` et `X_test_pad`. N'oubliez pas les arguments importants du padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test, dtype='float32', padding='post')\n",
    "\n",
    "\n",
    "assert(len(X_train_pad.shape) == 3)\n",
    "assert(len(X_test_pad.shape) == 3)\n",
    "assert(X_train_pad.shape[2] == 100)\n",
    "assert(X_test_pad.shape[2] == 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
